---
<!-- title: Home -->
feature_image: "https://images.unsplash.com/photo-1608322368442-2db3b4090724?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1040&q=80"
feature_text: |
 ## Yvette's Website
---
<!-- https://picsum.photos/1300/400?image=989 -->

js script
import './my-component.js';  


# This is my component  
<my-component></my-component>


<!-- ### Table of contents
>   1. [Education](#education)
>   2. [Experience](#experience)
>   3. [Certifications](#certifications)
>   4. [Skills](#skills)
>   5. [Publications](#publications)
>   6. [Projects](#projects) -->


[Education](#education) | [Experience](#experience) | [Certifications](#certifications)   
[Skills](#skills) | [Publications](#publications) | [Projects](#projects)


---

### Education <a name="education"></a>    
**National Taipei University**   
Master of Business Administration - MBA, Graduate Institute of Information Management   
Sep 2021 - Jun 2023

**Taipei Medical University**   
Bachelor of Science - BS, School of Food Safety   
Sep 2017 - Jun 2021

### Experience <a name="experience"></a>   
**Intern**   
DeepWave · Internship   
Jul 2022 - Present   
Da'an District, Taipei City, Taiwan   
Company Website：[https://dwave.cc/](https://dwave.cc/) 

**Graduate Research Assistant**   
National Taipei University   
Aug 2021 - Present   
Sanxia District, New Taipei City, Taiwan    

**Summer Internship**   
New Taipei City Government · Internship   
Jul 2020 - Aug 2020   
Banqiao District, New Taipei City, Taiwan   
Intern at Food and Drug Administration, Department of Health, New Taipei City Government   

### Certifications <a name="certifications"></a>   
**Amazon Web Services Cloud Practitioner**   
Issued Feb 2022 · Expires Feb 2025   

### Skills <a name="skills"></a>   
Cloud Computing  
Amazon Web Services (AWS)    
Machine Learning    
Natural Language Processing (NLP)   
Data Science   
Docker    
Python    
MySQL    
Scrum

### Publications <a name="publications"></a>   
**IMNTPU at the NTCIR-16 FinNum-3 Task: <a name="FinNum3"></a>   
Data Augmentation for Financial Numclaim Classification**   
NTCIR 16 Conference · Jun 14, 2022   
<!-- **Abstract**: This paper provides a detailed description of IMNTPU team at the NTCIR-16 FinNum-3 shared task in formal financial documents. We proposed the use of the XLM-RoBERTa-based model with two different approaches on data augmentation to perform the binary classification task in FinNum-3. The first run (i.e., IMNTPU-1) is our baseline through the fine-tuning of the XLM-RoBERTa without data augmentation. However, we assume that presenting different data augmentations may improve the task performance because of the imbalance in the dataset. Accordingly, we presented double redaction and translation methods on data augmentation in the second (IMNTPU-2) and third (IMNTPU-3) runs, respectively. The best macro-F1 scores obtained by our team in the Chinese and English datasets are 93.18% and 89.86%, respectively. The major contribution of this study provides a new understanding of data augmentation approach for the imbalanced dataset, which may help reduce the imbalanced situation in the Chinese and English datasets.    -->
<img src="https://user-images.githubusercontent.com/82231499/173595390-937d8a24-0c1a-4865-a78b-171547e8ea7f.png" width="15" height="15">
<!-- [[Pdf]](https://research.nii.ac.jp/ntcir/workshop/OnlineProceedings16/pdf/ntcir/08-NTCIR16-FINNUM-TengY.pdf)[[Poster]]() -->
<a href="https://research.nii.ac.jp/ntcir/workshop/OnlineProceedings16/pdf/ntcir/08-NTCIR16-FINNUM-TengY.pdf" target="_blank">[Pdf]</a>
<a href="" target="_blank">[Poster]</a>

<p>
  <a class="btn btn-primary" data-bs-toggle="collapse" href="#collapseExample" role="button" aria-expanded="false" aria-controls="collapseExample">
    Abstract
  </a>
  </button>
</p>
<div class="collapse" id="collapseExample">
  <div class="card card-body">
    This paper provides a detailed description of IMNTPU team at the NTCIR-16 FinNum-3 shared task in formal financial documents. We proposed the use of the XLM-RoBERTa-based model with two different approaches on data augmentation to perform the binary classification task in FinNum-3. The first run (i.e., IMNTPU-1) is our baseline through the fine-tuning of the XLM-RoBERTa without data augmentation. However, we assume that presenting different data augmentations may improve the task performance because of the imbalance in the dataset. Accordingly, we presented double redaction and translation methods on data augmentation in the second (IMNTPU-2) and third (IMNTPU-3) runs, respectively. The best macro-F1 scores obtained by our team in the Chinese and English datasets are 93.18% and 89.86%, respectively. The major contribution of this study provides a new understanding of data augmentation approach for the imbalanced dataset, which may help reduce the imbalanced situation in the Chinese and English datasets.
  </div>
</div>


### Projects <a name="projects"></a>   
**Contest | NTCIR16 FinNum3**   
Dec 2021 - Jun 2022   
This competition conducts a fine-grained analysis of numbers in professional financial documents (law-speaking conferences or financial analysis books), and uses binary classification to subdivide whether the numbers in the sentence are numbers analyzed by experts for professional financial documents, and use Macro F1-score to evaluate model performance. We won first (93.18%) and third (89.86%) in Chinese and English datasets respectively. In addition, a conference paper was published at the NTCIR-16 Conference entitled "[IMNTPU at the NTCIR-16 FinNum-3 Task: Data Augmentation for Financial Numclaim Classification](#FinNum3)".
